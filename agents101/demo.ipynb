{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19526d07",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1c692",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.tools import Tool\n",
    "\n",
    "from wikipedia_tools import save_results_to_path, read_results_from_path, get_top_k_keywords\n",
    "from wikipedia_tools import __all__ as tools_available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfbf611",
   "metadata": {},
   "source": [
    "## Environment Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabf6c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_save_format: str = \"\"\"Question: {user question}\\nTopic: {topic}\\nSummary: {summary of results}\"\"\" # format we expect agent to follow when saving results\n",
    "system_prompt = f\"\"\"You are an expert researcher that leverages wikipedia to answer questions. You can use the following tools: {tools_available}. You are not to answer questions without using a tool or using information resulting from a tool. For example, if you are asked \"What is the capital of France?\", you should use the \"search_wikipedia\" tool to find the answer. If a user later asks, \"What is the capital of France?\", you may simply answer based off of your previous findings. In summary, query for new questions, for redundant questions, use your previous findings.\n",
    "\n",
    "Output guidelines:\n",
    "You must always respond in the following JSON format:\n",
    "{{“intent”:…, “status”: ..., “function”: {{ “name”:…, “parameters”:...}}}}\n",
    "- The \"intent\" field should be a short description of what you are trying to do.\n",
    "- The \"status\" field should be one of the following: \"in_progress\", \"done\", or \"request_human_approval\".\n",
    "  - \"in_progress\" means you are still working on the task and will need to call another tool.\n",
    "  - \"done\" means you have completed the task and have all the information you need to answer the user's question.\n",
    "  - \"request_human_approval\" means you need human approval before proceeding.\n",
    "- The \"function\" field should contain the name of the tool being called and its parameters.\n",
    "  - The parameters should be a dictionary containing the necessary arguments for the tool. For example, \"parameters\": {{\"query\": \"What is the capital of France?\"}}.\n",
    "\n",
    "File saving format:\n",
    "For any requests that require saving the results the format should be as followed:\n",
    "{file_save_format}.\n",
    "- If the user does not specify a specific file name, you must name the file with the convention wiki_findings_i.txt\"\"\"\n",
    "\n",
    "questions: list[str] = [\n",
    "  \"What is the status of the Sudanese Civil War in August 2025?\",\n",
    "  \"Can you save your findings to ./sudan_civil_war.txt?\",\n",
    "  \"Tell me the exact contents of the file at ./random_txt.txt.\",\n",
    "  \"What are the top 3 words used in the previous text?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9c6e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_model = OllamaLLM(model=\"llama3.1\") \n",
    "basic_model2 = OllamaLLM(model=\"llama3.1\")\n",
    "basic_model = OllamaLLM(model=\"llama3.1\") \n",
    "\n",
    "tools = [\n",
    "  Tool(name=\"search_wikipedia\", func=wiki_tool.run, description=\"Searches wikipedia for the topic you provide\"), # Searches wikipedia\n",
    "  save_results_to_path, # save agents results to user specified path\n",
    "  read_results_from_path, # read agents results from user specified path\n",
    "  get_top_k_keywords \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f231ee92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_295873/284988254.py:1: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  agent = initialize_agent(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "ZeroShotAgent does not support multi-input tool save_results_to_path.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m agent = \u001b[43minitialize_agent\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m  \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbasic_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m  \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m  \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m  \u001b[49m\u001b[43magent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mzero-shot-react-description\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# ReAct-style\u001b[39;49;00m\n\u001b[32m      6\u001b[39m \u001b[43m  \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# uses ReAct concept\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:190\u001b[39m, in \u001b[36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    188\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    189\u001b[39m     emit_warning()\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain/agents/initialize.py:78\u001b[39m, in \u001b[36minitialize_agent\u001b[39m\u001b[34m(tools, llm, agent, callback_manager, agent_path, agent_kwargs, tags, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m     agent_cls = AGENT_TO_CLASS[agent]\n\u001b[32m     77\u001b[39m     agent_kwargs = agent_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m     agent_obj = \u001b[43magent_cls\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_llm_and_tools\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     79\u001b[39m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43magent_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     83\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m agent_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     85\u001b[39m     agent_obj = load_agent(\n\u001b[32m     86\u001b[39m         agent_path,\n\u001b[32m     87\u001b[39m         llm=llm,\n\u001b[32m     88\u001b[39m         tools=tools,\n\u001b[32m     89\u001b[39m         callback_manager=callback_manager,\n\u001b[32m     90\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain/agents/mrkl/base.py:139\u001b[39m, in \u001b[36mZeroShotAgent.from_llm_and_tools\u001b[39m\u001b[34m(cls, llm, tools, callback_manager, output_parser, prefix, suffix, format_instructions, input_variables, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_llm_and_tools\u001b[39m(\n\u001b[32m    114\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    123\u001b[39m     **kwargs: Any,\n\u001b[32m    124\u001b[39m ) -> Agent:\n\u001b[32m    125\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct an agent from an LLM and tools.\u001b[39;00m\n\u001b[32m    126\u001b[39m \n\u001b[32m    127\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    137\u001b[39m \u001b[33;03m        kwargs: Additional parameters to pass to the agent.\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m     \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_tools\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    140\u001b[39m     prompt = \u001b[38;5;28mcls\u001b[39m.create_prompt(\n\u001b[32m    141\u001b[39m         tools,\n\u001b[32m    142\u001b[39m         prefix=prefix,\n\u001b[32m   (...)\u001b[39m\u001b[32m    145\u001b[39m         input_variables=input_variables,\n\u001b[32m    146\u001b[39m     )\n\u001b[32m    147\u001b[39m     llm_chain = LLMChain(\n\u001b[32m    148\u001b[39m         llm=llm,\n\u001b[32m    149\u001b[39m         prompt=prompt,\n\u001b[32m    150\u001b[39m         callback_manager=callback_manager,\n\u001b[32m    151\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain/agents/mrkl/base.py:163\u001b[39m, in \u001b[36mZeroShotAgent._validate_tools\u001b[39m\u001b[34m(cls, tools)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_validate_tools\u001b[39m(\u001b[38;5;28mcls\u001b[39m, tools: Sequence[BaseTool]) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m     \u001b[43mvalidate_tools_single_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tools) == \u001b[32m0\u001b[39m:\n\u001b[32m    165\u001b[39m         msg = (\n\u001b[32m    166\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot no tools for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. At least one tool must be provided.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    167\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/langchain/agents/utils.py:19\u001b[39m, in \u001b[36mvalidate_tools_single_input\u001b[39m\u001b[34m(class_name, tools)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tool.is_single_input:\n\u001b[32m     18\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not support multi-input tool \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtool.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[31mValueError\u001b[39m: ZeroShotAgent does not support multi-input tool save_results_to_path."
     ]
    }
   ],
   "source": [
    "agent = initialize_agent(\n",
    "  llm=basic_model, \n",
    "  tools=tools, \n",
    "  prompt=system_prompt,\n",
    "  agent=\"zero-shot-react-description\",  # ReAct-style\n",
    "  verbose=True,\n",
    "  ) # uses ReAct concept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f2fc9d",
   "metadata": {},
   "source": [
    "### Planning agent example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf5a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who are the parties involved in the sudanese civil war? What is the historical context of the conflict? Provide a 200 word summary and save the results to ./sudan_historical_analysis.txt.\"\n",
    "response = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": query}]})\n",
    "print(f\"Question: {query}\\nResponse: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5940193c",
   "metadata": {},
   "source": [
    "### Agent with long-term memory example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bd605",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_store = FAISS(embedding_model)\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=250,       # number of characters per chunk\n",
    "    chunk_overlap=25,      # overlap between chunks to maintain context\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cf9ae",
   "metadata": {},
   "source": [
    "#### Creating vector store that can retrieve relavent chunks to a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9cf371",
   "metadata": {},
   "outputs": [],
   "source": [
    "sudan_text = read_results_from_path.func(\"./sudan_civil_war.txt\")\n",
    "chunks = text_splitter.split_text(sudan_text)\n",
    "docs = [Document(page_content=chunk) for chunk in chunks]\n",
    "for doc in docs:\n",
    "    vector_store.add_document(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494437b2",
   "metadata": {},
   "source": [
    "#### RAG assisted response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1aabd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are the main causes of the Sudanese Civil War?\"\n",
    "relevant_docs = vector_store.similarity_search(query, k=2)\n",
    "query_with_docs = f\"\"\"{query}. The following are documents from previous questions: {relevant_docs}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c196708d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = basic_model2.invoke({\"input\": query_with_docs})\n",
    "print(f\"Question: {query}\\nResponse: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rpai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
